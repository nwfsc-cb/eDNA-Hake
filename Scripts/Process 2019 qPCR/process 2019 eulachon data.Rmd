---
title: "Process 2019 eDNA Data- Eulachon"
author: "Owen R. Liu"
date: "2023-10-16"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(marmap)
library(ggplot2)
library(ggpubr)
library(rstan)
library(lubridate)
library(gridExtra)
library(sf)
library(brms)
library(loo)
library(here)
library(rnaturalearth)
library(viridis)
library(units)
library(terra)
options(dplyr.summarise.inform=FALSE)

# Working directories
# base.dir <- here()
# data.dir <- here('Data')
# plot.dir <- here("Plots and figures")
# script.dir <- here('Scripts','Owen Scripts')
```

# Purpose

The goal of this script is to clean processed qPCR eDNA data from the 2021 hake survey. Overall, we clean the eDNA qPCR results, join location and spatial covariate data from related datasets, and join data from the qPCR standard curves. In the second major portion of the processing, we produce forms of the data that will be used in the Stan model of hake biomass.

## Pre-processing Model Choices

There are choices to be made up front about the structure of the model that will be produced to fit in Stan. We keep track of those here at the top of the workflow, so that this script could  (hopefully, theoretically) be generalized for other species that were analyzed (i.e., lamprey and eulachon in addition to hake)

```{r model choices}
## DECLARED SPECIES OF INTEREST
SP <- "eulachon" # options: hake, lamprey, eulachon
###########################################################################
# Make a choice about the kind of model to run.
# Options are "Base", "lat.long.smooth", "lat.long.smooth.base"
MODEL.TYPE = "lat.long.smooth"
###########################################################################
# identifier
MODEL.ID <- "4_10_fix_nu_T-FIN"
###########################################################################
# variance scenario # options are "Base_Var", "Linear_Var"
MODEL.VAR <- "Base_Var" #
###########################################################################
NO.SURFACE <- "FALSE"

###########################################################################
# Threshold above which we consider a sample inhibited
INHIBIT.LIMIT <- 0.5

#set.seed(111)
# Construct smoothes for each 
# define knots.
N.knots.lon  <- 4
N.knots.lat  <- 10
N.knots.bd <- 5
N.knots.depth <- 4

######################################################
if(MODEL.TYPE == "Base"){
  TRIM.25 <- FALSE
} 
if(MODEL.TYPE == "lat.long.smooth"|MODEL.TYPE == "lat.long.smooth.base"){
  TRIM.25 <- TRUE
}  
```

# Import Data

First, we import "raw" forms of the data.

```{r import raw data}
# Pull in qPCR data, qPCR standards, sample id information
dat.all <- read_csv(here('Data','qPCR','Hake eDNA 2019 qPCR results 2023-02-10 results.csv'))
dat.stand <- read_csv(here('Data','qPCR','Hake eDNA 2019 qPCR results 2020-01-04 standards.csv'))
dat.sample.id <- read_csv(here('Data','qPCR','Hake eDNA 2019 qPCR results 2023-02-10 sample details.csv'))
dat.station.id <- read_csv(here('Data','CTD_hake_data_10-2019.csv'))

# Acoustic trawl data (DON'T NEED FOR EULACHON)
# dat.acoustic.aged <- read_csv(here('Data','Biomass','unkriged_aged_output_2021_combined_male_female.csv'),skip = 1)
# dat.acoustic.all.ages <- read_csv(here('Data','Biomass','unkriged_integrated_output_2021_combined_male_female.csv'))
```

# Process CTD

First, process the location and sample identifier information from CTD casts.

```{r clean ctd}
# Look at the data
glimpse(dat.station.id)
dat.station.id <- dat.station.id %>% 
  rename(station_id=`Station #`,date=Date, time=Time,lat=`SciGPS-Lat`,lon=`SciGPS-Lon`) %>% 
  mutate(date=as_date(date,format="%m/%d/%y")) %>% 
  mutate(year=year(date),month=month(date),day=day(date)) %>% 
  dplyr::select(station_id,Button,date,time,lat,lon,year,month,day) %>% 
  
  #fix lat/lon

  mutate(Lat=substr(lat,1,nchar(lat)-1),Lon=substr(lon,1,nchar(lon)-1)) %>%
  mutate(lat.deg=as.numeric(substr(Lat,1,2)),lon.deg = as.numeric(substr(Lon,1,3)),
         lat.dec=as.numeric(substr(Lat,3,nchar(Lat)))/60,lon.dec=as.numeric(substr(Lon,4,nchar(Lon)))/60) %>%
  mutate(lat = lat.deg + lat.dec, lon= -(lon.deg + lon.dec)) %>% 
  dplyr::select(-Lat,-Lon,-lat.deg,-lat.dec,-lon.deg,-lon.dec)

glimpse(dat.station.id)
```

```{r}
dat.station.filt <- dat.station.id %>% 
  filter(Button == "CTD at Depth" | station_id == "49-9") %>% 
  mutate(station = case_when(station_id=="77-0" ~ "77-MT505",
                             station_id=="78-0" ~ "78-MT506",
                             grepl("CTD ",station_id) ~ substr(station_id,5,nchar(station_id)),
                             grepl("CTD",station_id) ~ substr(station_id,4,nchar(station_id)),
                             TRUE ~ station_id) ) %>%
  mutate(transect=substr(station,1,2),
         transect=case_when(grepl("-",transect) ~ substr(transect,1,1),
                            TRUE ~ transect))
# Duplicate station IDs
dat.station.filt %>% count(station_id) %>% 
  filter(n>1)

#for now just filter so we keep just the first cast
dat.station.filt <- dat.station.filt %>% 
  group_by(station_id) %>% 
  slice(1) %>% 
  ungroup()
# now, all station IDs in this table are unique (1 row per station_ID)
```


Add a transect identifier
```{r NA stations}
## NAs introduced here- okay because some obs aren't associated with a transect(e.g., calibration controls)
station.NAs <- dat.station.filt %>% filter(is.na(transect))
station.NAs
```
## MarMap Depth
Attach depth from NOAA bathymetry

```{r bathy}
### Go get bathymetric data from NOAA to overlay on the transects.
limits.for.map <- dat.station.filt %>% 
  st_as_sf(coords=c('lon','lat'),crs=4326) %>% 
  st_bbox()+c(-1,-1,1,1) #add an extra degree on each side for buffer
  

b = getNOAA.bathy(lon1 = limits.for.map["xmin"],
          lon2 = limits.for.map[ "xmax" ],
          lat1 = limits.for.map["ymin"],
          lat2 = limits.for.map["ymax"],
          resolution = 1,keep = TRUE)

# THE BELOW CODE IS FOR THE ACOUSTIC TRANSECTS
# # find the first and last lat-lons from dat.acoustic data frame for each transect.
# dat.bathy <- dat.station.id %>% 
#   group_by(transect) %>% 
#   summarise(across(c("lon","lat"), .fns = list( Max = ~ max(.x)  ,
#                            Min = ~ min(.x) ,
#                            Mean = ~ mean(.x)))) %>% 
#   ungroup() %>% 
#   filter(!is.na(transect))
# 
# pull_transect_bathy <- function(lon_Min,lon_Max,lat_Mean,transect){
#   dat.trans <- get.transect(b,
#                           x1=lon_Max,x2=lon_Min,
#                           y1=lat_Mean,y2=lat_Mean,distance=TRUE)
#   dat.trans %>% 
#     mutate(transect=transect) %>% 
#     bind_rows(tibble(lon=-99,lat=-99,dist.km=0,depth=min(dat.trans$depth),transect=transect))
# }
# 
# bathy.transects <- purrr::pmap_df(dplyr::select(dat.bathy,transect,lon_Min,lon_Max,lat_Mean,transect),pull_transect_bathy)
```

```{r get sample depth}
# We can pull depth from each sample point
stations.bathy <- get.depth(b,x=dat.station.filt %>% 
                              dplyr::select(lon,lat),locator=F) %>% 
  rename(bathy.bottom.depth=depth) %>% 
  distinct()

dat.station.filt <- dat.station.filt %>% left_join(stations.bathy,by=join_by(lat,lon))

# temp <- marmap::get.depth(b,
#                           x= dat.station.id %>% dplyr::select(lon,lat),
#                           locator = FALSE)  
# dat.station.id$bathy.bottom.depth <-  -temp$depth
# 
# d.temp <- dat.station.id %>% group_by(date,year,month,day,station, transect) %>%
#   summarise(m.lat=mean(lat),m.lon=mean(lon),
#             m.water.depth=mean(water.depth),bathy.depth = mean(bathy.bottom.depth)) %>%
#   rename(lat=m.lat,lon=m.lon, water.depth=m.water.depth, bathy.bottom.depth=bathy.depth)
# # come up with consensus bottom depth
# d.temp$bottom.depth.consensus <- d.temp$water.depth
# d.temp <- d.temp %>% mutate(
#   bottom.depth.consensus = ifelse(water.depth<100 & bathy.bottom.depth>1000,bathy.bottom.depth,bottom.depth.consensus),
#   bottom.depth.consensus = ifelse(water.depth<600 & water.depth>400 & bathy.bottom.depth>1000,bathy.bottom.depth,bottom.depth.consensus),
#   bottom.depth.consensus = ifelse(water.depth<1100 & water.depth>1000 & bathy.bottom.depth>2000,bathy.bottom.depth,bottom.depth.consensus),
#   bottom.depth.consensus = ifelse(water.depth<0,bathy.bottom.depth,bottom.depth.consensus),
#   bottom.depth.consensus = ifelse(water.depth>1000 & bathy.bottom.depth<200,bathy.bottom.depth,bottom.depth.consensus))
# 
# dat.station.id.trim <- dat.station.id %>% dplyr::select(date,year,month,day, station, transect) %>%
#   # this combines the latitude and longitude to make a single concensus value for each Station.
#   left_join(d.temp,.)
# # get rid of a small number of duplicate stations in this data.frame.
# A <- dat.station.id.trim %>% group_by(station) %>% summarise(N=length(date)) %>% filter(N>1)
# THESE  <- A$station[A$station!=""]
# temp   <- dat.station.id.trim %>% filter(station %in% THESE) %>% arrange(station)
# #pull out every other row to drop duplicates.
# temp <- temp[seq(1,nrow(temp),by=2),]
# 
# dat.station.id.trim <- dat.station.id.trim %>% filter(!station %in% THESE) %>% bind_rows(.,temp)
# 
# ##
# dat.sample.control.id <- dat.sample.id %>% mutate(date= as.Date(Date.UTC,"%m/%d/%y"),year=year(date),month=month(date),day=day(date)) %>%
#   dplyr::select(sample=Tube..,date,year,month,day,drop.sample,field.negative.type,volume = water.filtered.L,)
# dat.sample.id  <- dat.sample.id %>% dplyr::select(sample=Tube..,
#                                                   station=CTD.cast,
#                                                   Niskin,
#                                                   depth,
#                                                   drop.sample,
#                                                   field.negative.type,
#                                                   volume = water.filtered.L,
#                                                   Fluor,
#                                                   Zymo=Zymo.columns)
```
Clean up columns of sample ID table

```{r}
dat.sample.id  <- dat.sample.id %>% 
  dplyr::select(sample=`Tube #`,
                station=`CTD cast`,
                Niskin,
                depth,
                drop.sample,
                field.negative.type,
                volume = water.filtered.L)
```

# Projection Grid Info

Pull in and create spatial grid to project on.

## 5km Grid Depth

```{r}
# Blake's raster with just grid cell ID numbers
dat_raster=rast(here('Data','raster_grid_blake','fivekm_grid.tif'))

# Depth associated with grid cell IDs
raster_depth <- read_csv(here('Data','raster_grid_blake','weighted_mean_NGDC_depths_for_5km_gridcells.csv')) %>% 
  rename(depth_m=WM_depth_m)

# join depths to cell numbers
cellnums <- tibble(rastID=as.numeric(values(dat_raster))) %>% 
  mutate(rastcell=row_number()) %>% 
  left_join(raster_depth,by=c("rastID"="Gridcell_ID"))

# make bathy raster
rast_depth <- setValues(dat_raster,cellnums$depth_m)
plot(rast_depth)
```
Convert spatial reference for station IDs to Blake's custom projection

```{r}
dat.station.id.proj <- dat.station.filt %>% 
  # convert to sf object for quick and easy spatial transformation
  st_as_sf(coords=c('lon','lat'),crs=4326,remove=F) %>% 
  # project to the same CRS as Blake's raster
  st_transform(crs(dat_raster)) %>% 
  mutate(utm.lon.m=st_coordinates(.)[,1],
         utm.lat.m=st_coordinates(.)[,2]) %>% 
  # convert back to a non-spatial df
  st_set_geometry(NULL)
glimpse(dat.station.id.proj)
```

## Distance Along Transect

Calculate a distance along each individual transect, starting from the observation closest to shore/shallowest

```{r}
transect_dists <- dat.station.id.proj %>% 
  distinct(transect,utm.lon.m,utm.lat.m,.keep_all = T) %>% 
  filter(!is.na(transect))

# function to calculate this for a subset of rows representing 1 transect
calc_transect_distance <- function(transect_df){
  transect_sf <- transect_df %>% 
    # arrange by longitude
    arrange(desc(utm.lon.m)) %>% 
    st_as_sf(coords=c('utm.lon.m','utm.lat.m'),crs=crs(dat_raster))
  # transect start is assumed to be the observation with the eastern-most longitude
  transect_start <- transect_sf %>% slice_head(n=1)
  # calculate distances in meters from the start
  dists <- st_distance(transect_sf,transect_start)
  
  # add the distances back to the transect data and return
  transect_sf %>% 
    mutate(transect_dist_m=dists[,1]) %>% 
    st_set_geometry(NULL)
}

# apply
transect_dists <- transect_dists %>% 
  group_split(transect) %>% 
  purrr::map_df(calc_transect_distance)

# join to other samples (incl. non-transect samples like controls)
dat.station.id.proj <- dat.station.id.proj %>% 
  left_join(transect_dists,by=join_by(station_id,station,Button, date, time, lat, lon, year, month, day, transect, bathy.bottom.depth))
```

# Join Samples and Stations

Join the sample ID and station ID information that we have cleaned so far. First pull out some degenerate rows.

```{r filter sampleid}
# Filter sample ID info for those obs associated with a direct station, and those that aren't (e.g., different kinds of calibration and controls)
samples.no.station <- dat.sample.id %>% 
  filter(station=="N/A"|station=="-")
glimpse(samples.no.station)
dat.sample.id.filt <- dat.sample.id %>% 
  filter(!(station=="N/A"|station=="-"))
```

```{r filter stationid}
# Do the same for the station ID dataframe
# unique(dat.station.id.proj$station_id)
station.id.NAs <- dat.station.id.proj %>% 
  filter(is.na(station))
glimpse(station.id.NAs)
dat.station.id.filt <- dat.station.id.proj %>% 
  filter(!is.na(station))
```

Now join the two filtered datasets

```{r}
dat.id <- dat.sample.id.filt %>%
  left_join(dat.station.id.filt,by="station") %>%
  mutate(depth = case_when(depth=="-" ~ "-99",
                           depth=="sfc" ~ "0",
                           depth=="300/150" ~ "300",
                           depth=="" ~ "0",
                           TRUE ~depth))

# check the many-to-many matches
# matchcheck <- dat.id %>% group_by(sample) %>% mutate(nobs=n()) %>% filter(nobs>1)
```

# Process Acoustic Data (SKIP FOR EULACHON)

For hake, we join the cleaned acoustic trawl survey data. There are two versions of these data- one is age-structured and the other is bulk biomass estimates per grid cell. We will use the age-structured data, but bring in some covariate identifiers from the other dataset.

```{r acoustic data,eval=F}
# Take a look at the data
glimpse(dat.acoustic.aged)
glimpse(dat.acoustic.all.ages)

# # using the aged data, separate age 1 from age 2+
# # Actually, the age 1s aren't in here.
# dat.acoustic <- dat.acoustic.aged %>%
#   pivot_longer(`1`:`20`,names_to = 'age',values_to='wgt') %>% 
#   mutate(age=as.integer(age)) %>%
#   # seaparate age 1s from age 2+
#   mutate(age_cat=ifelse(age==1,'age1',"age2plus")) %>% 
#   group_by(Transect,Lat,Lon,stratum,wgt_total,age_cat) %>% 
#   summarise(wgt=sum(wgt)) %>% 
#   dplyr::select(transect=Transect,
#                 lon=Lon, lat=Lat,
#                 stratum,wgt_total,
#                 age_cat,
#                 wgt) %>% 
#   pivot_wider(names_from = age_cat,values_from = wgt) %>% 
#   ungroup()
# 
# # bring in other identiefiers/covariates/sample info from the combined ages file
# dat.acoustic.obs.id <- dat.acoustic.all.ages %>% 
#   dplyr::select(transect=Transect,
#                 region=`Region no.`,
#                 VL_start,VL_end,
#                 lat=Lat,lon=Lon,
#                 stratum,
#                 depth_acoustic=Depth,
#                 NASC:nwgt_total,
#                 transect_spacing=`Transect spacing`,
#                 interval,
#                 mix_coef,sig_b,wgt_per_fish)
# 
# # join
# dat.acoustic <- dat.acoustic %>% 
#   left_join(dat.acoustic.obs.id,by= join_by(transect, lon, lat, stratum, wgt_total))
# glimpse(dat.acoustic)
```
```{r,eval=F}
dat.acoustic <- dat.acoustic.all.ages %>%
  rename(transect=Transect,
         lat=Lat,
         lon=Lon,
         mean.depth.m=`Layer Depth`,
         layer.thickness.m=`Layer height`,
         abundance=ntk_total,
         biomass_kg=wgt_total,
         abund_dens_n_nm2 = nntk_total,
         weight_dens_kg_nm2 = nwgt_total
         ) %>%
  dplyr::select(transect,
                VL_start,
                VL_end,
                lat,
                lon,
                mean.depth.m,
                layer.thickness.m,
                abundance,
                biomass_kg,
                abund_dens_n_nm2 ,
                weight_dens_kg_nm2,
                sig_b,
                NASC) %>% 
  arrange(transect,VL_start)
```

## Distance Along Transect

```{r acoustic ids,eval=F}
# make a sample ID number ("transect_number")
dat.acoustic <- dat.acoustic %>% 
  group_by(transect) %>% 
  mutate(N=1:n()) %>% 
  unite(trans_ID,transect,N,sep="_",remove=F) %>% 
  ungroup()
```

Similar to how we did it for the eDNA samples above, calculate a distance along transect, starting form the eastern-most

```{r calc transect distance,eval=F}
# convert to Blake's CRS
dat.acoustic.proj <- dat.acoustic %>% 
  # convert to sf object for quick and easy spatial transformation
  st_as_sf(coords=c('lon','lat'),crs=4326,remove=F) %>% 
  # project to the same CRS as Blake's raster
  st_transform(crs(dat_raster)) %>% 
  mutate(utm.lon.m=st_coordinates(.)[,1],
         utm.lat.m=st_coordinates(.)[,2]) %>% 
  # convert back to a non-spatial df
  st_set_geometry(NULL)
  
# apply
acoustic_transect_dists <- dat.acoustic.proj %>% 
  distinct(transect,utm.lon.m,utm.lat.m,.keep_all = T) %>% 
  filter(!is.na(transect))

acoustic_transect_dists <- acoustic_transect_dists %>% 
  group_split(transect) %>% 
  purrr::map_df(calc_transect_distance)

# join back to the acoustic data
# join to other samples (incl. non-transect samples like controls)
dat.acoustic.proj <- dat.acoustic.proj %>% 
  left_join(acoustic_transect_dists,by=join_by(trans_ID, transect, VL_start, VL_end, lat, lon, mean.depth.m, layer.thickness.m, abundance, biomass_kg, abund_dens_n_nm2, weight_dens_kg_nm2, sig_b, NASC, N))

glimpse(dat.acoustic.proj)
```


# Process Standards

```{r}
glimpse(dat.stand)
# fix some columns
dat.stand <- dat.stand %>% 
  dplyr::select(qPCR,well,sample,IPC_Ct,inhibition_rate,contains("task"),contains(SP)) %>% 
  rename(Ct=paste0(SP,'_Ct'),
         copies_ul=paste0(SP,"_copies_ul"),
         task=paste0(SP,"_task"))

# data with task "UNKNOWN"
unknown.task <- dat.stand %>% filter(task=="UNKNOWN")
# data with undetermined Ct
undetermined.Ct <- dat.stand %>% filter(Ct=="Undetermined")

# mark these as "-99"
dat.stand <- dat.stand %>% 
  mutate(Ct=ifelse(Ct=="Undetermined","-99",Ct)) %>% 
  # add useful data transformed vars
  mutate(Ct=as.numeric(Ct),Ct_bin=ifelse(Ct>0,1,0),log10_copies=log10(copies_ul)) %>% 
  group_by(qPCR) %>% 
  # add an id number for each qPCR
  mutate(plate_idx=cur_group_id()) %>% 
  ungroup()
```

Count samples and make sure there are enough for each qPCR

```{r}
# Make sure there are sufficient samples for each qPCR
stand.summary <- dat.stand %>% count(qPCR,copies_ul,name="N")
glimpse(stand.summary)
```
# Process Samples

Organize the actual samples, including real samples, controls, and non-template controls
```{r}
dat.samp <- dat.all %>% 
  dplyr::select(qPCR,well,sample,IPC_Ct,inhibition_rate,task,Zymo,contains(SP)) %>%
  rename(Ct=paste0(SP,"_Ct"),copies_ul = paste0(SP,"_copies_ul")) %>%
  mutate(Ct=ifelse(Ct=="",-99,Ct),
         Ct=ifelse(Ct=="Undetermined",-99,Ct),
         Ct=ifelse(is.na(Ct),-99,Ct), 
         Ct=as.numeric(Ct)) %>%
  mutate(IPC_Ct = as.character(IPC_Ct),
        IPC_Ct=ifelse(IPC_Ct=="",-99,IPC_Ct),
        IPC_Ct=ifelse(IPC_Ct=="Undetermined",-99,IPC_Ct),
        IPC_Ct=ifelse(is.na(IPC_Ct)==T,-99,IPC_Ct), 
        IPC_Ct=as.numeric(IPC_Ct))

glimpse(dat.samp)
```
## Join IDs to Samples

```{r}
dat.samp <- dat.samp %>% 
  left_join(dat.id,by='sample') %>% 
  mutate(depth=as.numeric(depth)) %>% 
  mutate(depth_cat=case_when(depth < 25 ~ 0,
                            depth ==25 ~ 25,  
                            depth > 25  & depth <= 60  ~ 50,
                            depth > 60  & depth <= 100 ~ 100,
                            depth > 119 & depth <= 150 ~ 150,
                            depth > 151 & depth <= 200 ~ 200,
                            depth > 240 & depth <= 350 ~ 300,
                            depth > 400 & depth <= 500 ~ 500))

# no station info
dat.noID <- dat.samp %>% filter(is.na(station))

#
dat.samp <- dat.samp %>% 
  dplyr::select(date,time,lat,lon,year,month,day,transect,qPCR,well,sample,IPC_Ct,inhibition_rate,task,Zymo,Ct,copies_ul,depth,depth_cat,station,Niskin,volume,bathy.bottom.depth,utm.lon.m,utm.lat.m,transect_dist_m)
```

## Save

```{r}
write_rds(dat.samp,here('Data','eulachon qPCR 2019 joined cleaned 11_15_2023.rds'))
```


# Visualize

Let's make some plots and see what we've got
```{r plottheme}
# ggplot theme
plot_theme <-   theme_minimal()+
  theme(text=element_text(family="sans",size=10,color="black"),
        legend.text = element_text(size=10),
        axis.title=element_text(family="sans",size=10,color="black"),
        axis.text=element_text(family="sans",size=8,color="black"),
        panel.grid.major = element_line(color="gray50",linetype=3),
        panel.border = element_rect(fill=NA,color='black'))
theme_set(plot_theme)
```

```{r}
# spatial background map
# load west cost land for mapping
coast <- ne_states(country='United States of America',returnclass = 'sf') %>% 
  filter(name %in% c('California','Oregon','Washington','Nevada')) %>%
  st_transform(crs = crs(dat_raster))

bbox <- dat.id %>% 
  filter(!is.na(utm.lon.m)) %>% 
  st_as_sf(coords=c("utm.lon.m","utm.lat.m"),crs=crs(dat_raster)) %>% 
  st_bbox()

coast <- coast %>% st_crop(bbox)
```

## Maps of Samples by Depth

Surface

```{r}
dat.sfc <- dat.samp %>% 
  filter(depth_cat==0) %>% 
  mutate(is_inhibited=ifelse(inhibition_rate>0.5,"Inhibited","Non-Inhibited")) %>% 
  filter(!is.na(utm.lat.m))

dat.sfc.sf <- dat.sfc %>% 
  mutate(copies_ul=ifelse(copies_ul>10,10,copies_ul)) %>% 
  st_as_sf(coords=c("utm.lon.m","utm.lat.m"),crs=crs(dat_raster))

dat.sfc.p <- ggplot()+
  geom_sf(data=coast,fill='gray50')+
  geom_sf(data=dat.sfc.sf,aes(size=copies_ul),shape=1)+
  facet_wrap(~is_inhibited)+
  scale_color_viridis()
dat.sfc.p
```

50m

```{r}
dat.50 <- dat.samp %>% 
  filter(depth_cat==50) %>% 
  mutate(is_inhibited=ifelse(inhibition_rate>0.5,"Inhibited","Non-Inhibited")) %>% 
  filter(!is.na(utm.lat.m))

dat.50.sf <- dat.50 %>% 
  mutate(copies_ul=ifelse(copies_ul>10,10,copies_ul)) %>% 
  st_as_sf(coords=c("utm.lon.m","utm.lat.m"),crs=crs(dat_raster))

dat.50.p <- ggplot()+
  geom_sf(data=coast,fill='gray50')+
  geom_sf(data=dat.50.sf,aes(size=copies_ul),shape=1)+
  facet_wrap(~is_inhibited)+
  scale_color_viridis()
dat.50.p
```


150m

```{r}
dat.150 <- dat.samp %>% 
  filter(depth_cat==150) %>% 
  mutate(is_inhibited=ifelse(inhibition_rate>0.5,"Inhibited","Non-Inhibited")) %>% 
  filter(!is.na(utm.lat.m))

dat.150.sf <- dat.150 %>% 
  mutate(copies_ul=ifelse(copies_ul>10,10,copies_ul)) %>% 
  st_as_sf(coords=c("utm.lon.m","utm.lat.m"),crs=crs(dat_raster))

dat.150.p <- ggplot()+
  geom_sf(data=coast,fill='gray50')+
  geom_sf(data=dat.150.sf,aes(size=copies_ul),shape=1)+
  facet_wrap(~is_inhibited)+
  scale_color_viridis()
dat.150.p
```


200m

```{r}
dat.200 <- dat.samp %>% 
  filter(depth_cat==200) %>% 
  mutate(is_inhibited=ifelse(inhibition_rate>0.5,"Inhibited","Non-Inhibited")) %>% 
  filter(!is.na(utm.lat.m))

dat.200.sf <- dat.200 %>% 
  mutate(copies_ul=ifelse(copies_ul>10,10,copies_ul)) %>% 
  st_as_sf(coords=c("utm.lon.m","utm.lat.m"),crs=crs(dat_raster))

dat.200.p <- ggplot()+
  geom_sf(data=coast,fill='gray50')+
  geom_sf(data=dat.200.sf,aes(size=copies_ul),shape=1)+
  facet_wrap(~is_inhibited)+
  scale_color_viridis()
dat.200.p
```


300m

```{r}
dat.300 <- dat.samp %>% 
  filter(depth_cat==300) %>% 
  mutate(is_inhibited=ifelse(inhibition_rate>0.5,"Inhibited","Non-Inhibited")) %>% 
  filter(!is.na(utm.lat.m))

dat.300.sf <- dat.300 %>% 
  mutate(copies_ul=ifelse(copies_ul>10,10,copies_ul)) %>% 
  st_as_sf(coords=c("utm.lon.m","utm.lat.m"),crs=crs(dat_raster))

dat.300.p <- ggplot()+
  geom_sf(data=coast,fill='gray50')+
  geom_sf(data=dat.300.sf,aes(size=copies_ul),shape=1)+
  facet_wrap(~is_inhibited)+
  scale_color_viridis()
dat.300.p
```


500m

```{r}
dat.500 <- dat.samp %>% 
  filter(depth_cat==500) %>% 
  mutate(is_inhibited=ifelse(inhibition_rate>0.5,"Inhibited","Non-Inhibited")) %>% 
  filter(!is.na(utm.lat.m))

dat.500.sf <- dat.500 %>% 
  mutate(copies_ul=ifelse(copies_ul>10,10,copies_ul)) %>% 
  st_as_sf(coords=c("utm.lon.m","utm.lat.m"),crs=crs(dat_raster))

dat.500.p <- ggplot()+
  geom_sf(data=coast,fill='gray50')+
  geom_sf(data=dat.500.sf,aes(size=copies_ul),shape=1)+
  # none inhibited
  # facet_wrap(~is_inhibited)+
  scale_color_viridis()
dat.500.p
```

## Cross-sections by Transect

Transect reference
```{r}
transects.ref.samp <- dat.samp %>% 
  ggplot(aes(transect,lat))+
  geom_point()+
  labs(x="Transect Number",y="Latitude",title="Transect Reference, Samples")
transects.ref.samp

# transects.ref.acoustic <- dat.acoustic %>% 
#   ggplot(aes(transect,lat))+
#   geom_point()+
#   labs(x="Transect Number",y="Latitude",title="Transect Reference, Acoustic")
# transects.ref.acoustic
```

```{r}
plot_transect <- function(df,transect_id){
  dfsub <- df %>% 
    filter(transect==transect_id) %>% 
    mutate(transect_dist_km=as.numeric(transect_dist_m/1000))
  outp <- dfsub %>% 
    ggplot(aes(transect_dist_km,depth_cat,size=log(copies_ul)))+
    geom_point(shape=1)+
    scale_y_reverse()+
    scale_x_reverse()+
    # scale_size_continuous(limits=c(-6,8),breaks=seq(-6,9,by=5))+
    labs(x="Distance Along Transect",y="Depth",size="Log Copies (ul)",title=paste0("Transect ",transect_id))
  outp
}
```

Plot the transect cross-sections

```{r,fig.width=10,fig.height=8}
transect.cross.sections <- purrr::map(unique(dat.samp$transect),plot_transect,df=dat.samp)

cowplot::plot_grid(plotlist=transect.cross.sections[1:9],nrow=3)
cowplot::plot_grid(plotlist=transect.cross.sections[10:19],nrow=3)
cowplot::plot_grid(plotlist=transect.cross.sections[20:29],nrow=3)
cowplot::plot_grid(plotlist=transect.cross.sections[30:39],nrow=3)
cowplot::plot_grid(plotlist=transect.cross.sections[40:42],nrow=1)
```
## Standards

Plot standard curves

```{r,fig.width=10,fig.height=10}
# stand.curve.plots <- purrr::map(unique(dat.stand$qPCR),function(s){
#   dat.stand %>% 
#     filter(qPCR==s,Ct!=-99) %>% 
#     ggplot(aes(log10_copies,Ct))+
#     geom_point()+
#     geom_smooth(method='lm',color='darkgreen')+
#     labs(title=paste0("qPCR: ",s))+
#     stat_regline_equation(label.x = 2, label.y = 33)
#   
# })
# stand.curve.plots[[1]]

stand.curve.plots <- dat.stand %>% 
  filter(Ct!=-99) %>% 
  ggplot(aes(log10_copies,Ct))+
  geom_point()+
  geom_smooth(method='lm',color='darkgreen')+
  facet_wrap(~qPCR,nrow=7,ncol=5)+
  stat_regline_equation(label.x = 2, label.y = 33)
stand.curve.plots
```
```{r all standards}
stand.curves.all <- dat.stand %>% 
  filter(Ct!=-99) %>% 
  ggplot(aes(log10_copies,Ct,color=qPCR))+
  geom_point()+
  guides(color='none')+
  geom_smooth(method='lm',se=F)
stand.curves.all

# amplification success?
qpcr_success <- dat.stand %>% 
  ggplot(aes(x=log10_copies ,y=Ct_bin,color=qPCR))+
  geom_jitter(alpha=0.75,width=0,height=0.05)+
  guides(color='none')+
  geom_smooth(method = "glm", 
    method.args = list(family = "binomial"), 
    se = FALSE)
qpcr_success
```


# Join Other Years

Join the 2019 data, which was processed in a slightly different way:

```{r}

```

