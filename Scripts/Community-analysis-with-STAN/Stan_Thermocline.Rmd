---
title: "STAN for Thermocline"
author: "Ramon Gallego"
date: "8/26/2021"
output: 
 html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Making a STAN model for hypothesis testing of metabarcoding data

We have one problem with the application of metabarcoding data for hypothesis testing - we can't really be sure if the weight we are putting on number of reads, sequencing depth data transformation and algorithm to estimate sample dissimilarity are the ones driving the change and therefore could mask the real biological effect.
I include below the libraries I used
```{r libraries, echo=T, message=FALSE, warning=FALSE}
library(here)
library (tidyverse)
library(rstan)
```


## The design

We have sampled eDNA from locations in the Pacific Coast, at different depths. At each location and depth, we collected up to two Niskin Bottles, and each of them was amplified and sequenced twice (some of them more, some less).

The idea is to get to a posterior probability of the proportion of DNA from each taxa  (*i*) at each lat,lon,depth point (*j*), taking into account the number of PCR cycles, the primer efficiency and the stochasticity of the PCR process. 

The Amount of DNA in the water from taxa *i* \(D_i\) is unknown but we can express it as function of the number of Amplicons detected \(A_i\) and the process that generates those: Amplification at the efficiency (\(\alpha\)) for a number of PCR cycles. We don't see all the amplicons we generate, only a proportion of them end up in the sequencing machine. \(\eta\)


\begin{equation}

 A_{ij} = C_{ij}(1+\alpha)^{N_{PCR}} \times  \eta 
 (\#eq:label)
\end{equation}

And in log space

\begin{equation}
log(A_{ij}) = log (C_{ij}) + N_{PCR} \times log(1+\alpha) + log (\eta)
\end{equation}

But we can express the original Counts as the proportion of DNA copies from species *i* in the sample so

\begin{equation}
 B_{ij} = \frac{C_{ij}}{\sum_{i}C_{ij}} 
\end{equation}

\begin{equation}
log(B_{ij}) = log(C_{ij}) - log(\sum_i C_{ij})
\end{equation}

And the last term is the same for all values from sample *j*.

So the equation that brings the 

Now we change \(A_{ij}\) - which assumes a deterministic process from a proportion of copies of DNA and number of PCR cycles, and thus make it \(\lambda_{ijk}\) where *k* is the particular PCR reaction from that *j* and the Observed values come from a negative binomial distribution

\begin{equation}
log(\lambda_{ijkl}) = log(B_{ijkl})  + N_{PCR} \times log(1+\alpha) + log (\eta_{jkl}) \\ Y_{ijkl} \sim {\sf NegativeBinomial}(\lambda_{ijkl}, \tau)

\end{equation}

And the variance is 
\begin{equation}
\lambda_{ijkl} + \frac{\lambda^2_{ijkl}}{\tau}
\end{equation}

As it is, we are assuming that PCRs from two Niskins from the same station reflect the same, (aka biological replicates are perfect). But we can bring the concentration of DNA at site *j* (a combination of lat, lon and depth) as: 

\begin{equation}
log(\lambda_{ijl}) = \gamma_{il} + \delta_{ijl} \\ \delta_{ijl} âˆ¼ Normal(0,\sigma^2)
\end{equation}

### Get the data, prepare the data

You remember from previous adventures how picky STAN is with regards to the structure of the data. starting with the tibbles with the metadata and the counts, and we'll move from there

```{r pressure, echo=FALSE, message=F}
abundance.table <- read_csv(here("Data",  "by.taxa.dataset.metadata.csv")) %>% mutate (Transect = as.character(Transect))

metadata  <- read_csv(here("Data", "Final_metadata.csv"))
```

